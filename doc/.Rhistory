setwd("~/Dropbox/Tian_Teaching/G5243-ADS/0-Projects-startercodes/3-Spring2017/Project3_PoodleKFC/doc")
setwd("./ads_spr2017_proj3")
experiment_dir <- "../data/zipcode/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
model_values <- seq(3, 11, 2)
model_labels = paste("GBM with depth =", model.values)
model_values <- seq(3, 11, 2)
model_labels = paste("GBM with depth =", model_values)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep=""),
header=F)
label_train <- as.numeric(unlist(label_train) == "9")
source("./lib/feature.R")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zipcode",
export=TRUE))
}
if(!require("EBimage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
library(EBimage)
library("EBimage")
biocLite("EBImage")
library("EBimage")
library(EBimage)
library("EBImage")
setwd("./ads_spr2017_proj3")
experiment_dir <- "../data/zipcode/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
model_values <- seq(3, 11, 2)
model_labels = paste("GBM with depth =", model_values)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep=""),
header=F)
label_train <- as.numeric(unlist(label_train) == "9")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zipcode",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_test_dir,
"test",
data_name="zip",
export=TRUE))
#save(dat_train, file="./output/feature_train.RData")
#save(dat_test, file="./output/feature_test.RData")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_test_dir,
"test",
data_name="zip",
export=TRUE))
}
#save(dat_train, file="./output/feature_train.RData")
#save(dat_test, file="./output/feature_test.RData")
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(depth_values), 2))
for(k in 1:length(depth_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(depth_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
library("EBImage")
library("gbm")
experiment_dir <- "../data/zipcode/" # This will be modified for different data sets.
img_train_dir <- paste(experiment_dir, "train/", sep="")
img_test_dir <- paste(experiment_dir, "test/", sep="")
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
model_values <- seq(3, 11, 2)
model_labels = paste("GBM with depth =", model_values)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep=""),
header=F)
label_train <- as.numeric(unlist(label_train) == "9")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir,
"train",
data_name="zip",
export=TRUE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_test_dir,
"test",
data_name="zip",
export=TRUE))
}
#save(dat_train, file="./output/feature_train.RData")
#save(dat_test, file="./output/feature_test.RData")
source("../lib/train.R")
source("../lib/test.R")
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
source("../lib/cross_validation.R")
if(run.cv){
err_cv <- array(dim=c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat_train, label_train, model_values[k], K)
}
save(err_cv, file="../output/err_cv.RData")
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.15))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2],depth_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.15))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
if(run.cv){
load("../output/err_cv.RData")
#pdf("../fig/cv_results.pdf", width=7, height=5)
plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
main="Cross Validation Error", type="n", ylim=c(0, 0.25))
points(model_values, err_cv[,1], col="blue", pch=16)
lines(model_values, err_cv[,1], col="blue")
arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2],
length=0.1, angle=90, code=3)
#dev.off()
}
model_best=model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[,1])]
}
par_best <- list(par=model_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
View(err_cv)
which.min(err_cv[,1])
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
fit_train <- train(dat_train, label_train, par_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par=par_best))
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par=par_best))
par_best$par
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
source('~/Dropbox/Tian_Teaching/G5243-ADS/0-Projects-startercodes/3-Spring2017/Project3_PoodleKFC/lib/train.R')
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
source('~/Dropbox/Tian_Teaching/G5243-ADS/0-Projects-startercodes/3-Spring2017/Project3_PoodleKFC/lib/train.R')
model_best=model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[,1])]
}
par_best <- list(depth=model_best)
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
save(fit_train, file="../output/fit_train.RData")
tm_test=NA
if(run.test){
load(file=paste0("../output/feature_", "zip", "_", "test", ".RData"))
load(file="../output/fit_train.RData")
tm_test <- system.time(pred_test <- test(fit_train, dat_test))
save(pred_test, file="../output/pred_test.RData")
}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for making prediction=", tm_test[1], "s \n")
if (!require("pacman")) install.packages("pacman")
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
require("pacman")
install.packages("pacman")
if (!require("pacman")) {
## Make sure your current packages are up to date
update.packages()
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
if (!require("pacman")) {
## Make sure your current packages are up to date
update.packages()
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages(c("acs", "backports", "bayesplot", "BH", "bit", "bit64", "blogdown", "bookdown", "boot", "broom", "cairoDevice", "car", "caTools", "checkmate", "choroplethr", "chron", "cli", "cluster", "coin", "colourpicker", "countrycode", "curl", "data.table", "dendextend", "devtools", "digest", "doParallel", "dplyr", "DT", "dtplyr", "dygraphs", "ellipse", "evaluate", "factoextra", "FactoMineR", "fansi", "fftwtools", "flexmix", "foreach", "foreign", "formatR", "Formula", "fpc", "gbm", "gdata", "gender", "geosphere", "ggplot2", "ggpubr", "ggrepel", "ggsci", "git2r", "gridExtra", "gtools", "highr", "Hmisc", "htmlTable", "htmlwidgets", "httpuv", "hunspell", "igraph", "inline", "irlba", "iterators", "janeaustenr", "kernlab", "knitr", "lattice", "lazyeval", "lme4", "lmtest", "loo", "mapproj", "maps", "maptools", "markdown", "MASS", "Matrix", "matrixStats", "mclust", "memoise", "mgcv", "mime", "miniUI", "modeltools", "multcomp", "munsell", "mvtnorm", "nloptr", "NLP", "NMF", "OAIHarvester", "openNLPdata", "openssl", "packrat", "pbkrtest", "pkgmaker", "PKI", "plotrix", "ps", "psych", "qdap", "qdapDictionaries", "qdapRegex", "qdapTools", "quantreg", "R6", "randomForest", "RANN", "raster", "Rcpp", "RcppEigen", "RCurl", "registry", "remotes", "reprex", "reshape2", "rgdal", "rgeos", "RgoogleMaps", "rJava", "rjson", "rlang", "rmarkdown", "rngtools", "robustbase", "rpart", "rpart.plot", "rprojroot", "rsconnect", "rscopus", "rstantools", "rstudioapi", "sandwich", "scales", "scatterplot3d", "selectr", "servr", "shiny", "shinyjs", "shinystan", "sm", "sourcetools", "sp", "SparseM", "statmod", "stringdist", "stringi", "stringr", "survival", "syuzhet", "testthat", "TH.data", "threejs", "tidyr", "tidyselect", "tidytext", "tm", "tokenizers", "topicmodels", "trimcluster", "viridis", "WDI", "withr", "wordcloud", "xlsx", "XML", "xml2", "xtable", "xts", "yaml", "zoo"))
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}y
install.packages("pacman")
library(packman)
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages("devtools")
install.packages("devtools")
install.packages("yaml")
install.packages(c("bit", "cairoDevice", "caTools", "chron", "curl", "data.table", "digest", "dplyr", "fansi", "foreign", "gbm", "ggplot2", "ggrepel", "git2r", "gtools", "igraph", "irlba", "kernlab", "lattice", "lme4", "lmtest", "mapproj", "maps", "maptools", "MASS", "Matrix", "matrixStats", "mclust", "mgcv", "mime", "mvtnorm", "nloptr", "NMF", "openssl", "ps", "quantreg", "randomForest", "RANN", "raster", "Rcpp", "RcppEigen", "RCurl", "rgdal", "rgeos", "rJava", "rjson", "rlang", "robustbase", "rstan", "rstanarm", "scales", "sentimentr", "slam", "sm", "sourcetools", "sp", "StanHeaders", "stringdist", "stringi", "stringr", "survival", "testthat", "tidyr", "tidyselect", "tm", "tokenizers", "wordcloud", "XML", "xml2", "xts", "zoo"))
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
install.packages("devtools")
library(devtools)
install.packages("base64enc")
install.packages("devtools")
install.packages("processx")
install.packages("devtools")
install.packages("backports")
install.packages("devtools")
install.packages("glue")
install.packages("devtools")
library(devtools)
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #97 files in total
### only process one of the files in the folder as an example, in your project, you need to use all the files
current_file_name <- sub(".txt","",file_name_vec[3])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])
## png folder is not provided on github (the code is only on demonstration purpose)
current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
### save tesseract text file
writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])
## png folder is not provided on github (the code is only on demonstration purpose)
current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
### save tesseract text file
writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
### only process one of the files in the folder as an example, in your project, you need to use all the files
current_file_name <- sub(".txt","",file_name_vec[5])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
## detect tesseract word error
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]] #1124 tokens
tesseract_if_clean <- unlist(lapply(tesseract_vec,ifCleanToken)) # source code of ifCleanToken in in lib folder
current_file_name
current_ground_truth_txt
current_tesseract_txt
clean_tesseract_txt
tesseract_vec
tesseract_if_clean
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
## devtools is required
library(devtools)
install_github("trinker/pacman")
}
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
file_name_vec <- list.files("../data/ground_truth") #100 files in total
for(i in c(1:length(file_name_vec))){
current_file_name <- sub(".txt","",file_name_vec[i])
## png folder is not provided on github (the code is only on demonstration purpose)
current_tesseract_txt <- tesseract::ocr(paste("../data/png/",current_file_name,".png",sep=""))
### clean the tessetact text (separate line by "\n", delete null string, transter to lower case)
clean_tesseract_txt <- strsplit(current_tesseract_txt,"\n")[[1]]
clean_tesseract_txt <- clean_tesseract_txt[clean_tesseract_txt!=""]
### save tesseract text file
writeLines(clean_tesseract_txt, paste("../data/tesseract/",current_file_name,".txt",sep=""))
}
### only process one of the files in the folder as an example, in your project, you need to use all the files
current_file_name <- sub(".txt","",file_name_vec[5])
## read the ground truth text
current_ground_truth_txt <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), warn=FALSE)
## read the tesseract text
current_tesseract_txt <- readLines(paste("../data/tesseract/",current_file_name,".txt",sep=""), warn=FALSE)
clean_tesseract_txt <- paste(current_tesseract_txt, collapse = " ")
## detect tesseract word error
tesseract_vec <- str_split(clean_tesseract_txt," ")[[1]] #1124 tokens
tesseract_if_clean <- unlist(lapply(tesseract_vec,ifCleanToken)) # source code of ifCleanToken in in lib folder
tesseract_vec
tesseract_if_clean
tesseract_delete_error_vec <- tesseract_vec[tesseract_if_clean] #1095
tesseract_delete_error_vec
ground_truth_vec <- str_split(paste(current_ground_truth_txt, collapse = " ")," ")[[1]] #1078
old_intersect_vec <- vecsets::vintersect(tolower(ground_truth_vec), tolower(tesseract_vec)) #607
new_intersect_vec <- vecsets::vintersect(tolower(ground_truth_vec), tolower(tesseract_delete_error_vec)) #600
OCR_performance_table <- data.frame("Tesseract" = rep(NA,4),
"Tesseract_with_postprocessing" = rep(NA,4))
row.names(OCR_performance_table) <- c("word_wise_recall","word_wise_precision",
"character_wise_recall","character_wise_precision")
OCR_performance_table["word_wise_recall","Tesseract"] <- length(old_intersect_vec)/length(ground_truth_vec)
OCR_performance_table["word_wise_precision","Tesseract"] <- length(old_intersect_vec)/length(tesseract_vec)
OCR_performance_table["word_wise_recall","Tesseract_with_postprocessing"] <- length(new_intersect_vec)/length(ground_truth_vec)
OCR_performance_table["word_wise_precision","Tesseract_with_postprocessing"] <- length(new_intersect_vec)/length(tesseract_delete_error_vec)
kable(OCR_performance_table, caption="Summary of OCR performance")
getwd()
for (each in 1:length(current_tesseract_txt)){print(nchar(each))}
for (each in 1:length(current_tesseract_txt)){print(nchar(current_tesseract_txt[each]))}
clean_tesseract_txt
for (each in 1:length(tesseract_vec)){print(nchar(tesseract_vec[each]))}
current_ground_truth_txt
?list.files
