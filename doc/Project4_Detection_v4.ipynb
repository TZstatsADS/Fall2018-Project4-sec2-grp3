{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project4_Detection_v4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "rViGVJ0xRK5u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Detection"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yfF_19MM9yWY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "596e109c-0b3a-465c-ca52-c7b601f3dbd0"
      },
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "replace data/ground_truth/group1_00000005.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iwpNpdFxUqll",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import collections\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "import itertools\n",
        "#import itertools\n",
        "#nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gw4gIzwQRDqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "cd25a72b-4775-488f-9964-63a1efcbf6f3"
      },
      "cell_type": "code",
      "source": [
        "##### read ground_truth\n",
        "ground_dir = glob.glob(os.path.join(os.getcwd(),'data/ground_truth','*.txt'))\n",
        "ground_tokens = []\n",
        "for gd in ground_dir:\n",
        "    with open(gd) as ground_file:\n",
        "        ground_raw = ground_file.read()\n",
        "        ground_t = ground_raw.split()   \n",
        "        ground_tokens += ground_t\n",
        "print(len(ground_tokens))\n",
        "print(ground_tokens[:20])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "294841\n",
            "['wa', 'reasonable,', 'although', 'it', 'substantially', 'increased', 'the', 'programs', 'and', 'authority', 'over', 'preset', 'law.', 'Following', 'is', 'a', 'brief', 'list', 'of', 'key']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zoq2wOeFRDqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tess_dir = glob.glob(os.path.join(os.getcwd(),'data/tesseract','*.txt'))\n",
        "#ground_dir = glob.glob(os.path.join(os.getcwd(),'data/ground_truth','*.txt'))\n",
        "file_name_gd = []\n",
        "file_name_td = []\n",
        "for gd, td in zip(ground_dir, tess_dir):\n",
        "        with open(gd, encoding=\"utf8\") as ground_file:    #, encoding=\"utf8\"\n",
        "            with open(td, encoding=\"utf8\") as tess_file:                \n",
        "                ground_r = list(ground_file.readlines()) \n",
        "                tess_r = list(tess_file.readlines())\n",
        "                if len(tess_r) == len(ground_r):\n",
        "                    file_name_td.append(td)\n",
        "                    file_name_gd.append(gd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rz5JErxMRDqU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make sure all the lines have the same size.\n",
        "ground_tokens=[]\n",
        "tess_tokens=[]\n",
        "for gd, td in zip(file_name_gd, file_name_td):\n",
        "        with open(gd) as file1:\n",
        "            with open(td) as file2:\n",
        "                for line1,line2 in zip(file1,file2):\n",
        "                    if len(line1)==len(line2):\n",
        "                        for word1,word2 in zip(line1.split(),line2.split()):\n",
        "                            ground_tokens.append(word1)\n",
        "                            tess_tokens.append(word2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xs2RfLUgXj6e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5c7d5846-0d4f-4a99-806d-59b4aa314f19"
      },
      "cell_type": "code",
      "source": [
        "print(tess_tokens[:30])\n",
        "print(ground_tokens[:30])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ha', 'reas-', 'able,', 'although', '1:', 'substantlally', '1ncreased', 'the', 'programs', 'and', 'authorlty', 'preset', '1-H.', 'Followl', '.', '15', 'a', 'brlef', '115:', 'of', 'key', 'lssues', 'that', 'were', 'consldered', 'and', 'the', 'whlch', 'm-Ior', 'and']\n",
            "['wa', 'reasonable,', 'although', 'it', 'substantially', 'increased', 'the', 'programs', 'and', 'authority', 'over', 'preset', 'law.', 'Following', 'is', 'a', 'brief', 'list', 'of', 'key', 'issues', 'that', 'were', 'considered', 'and', 'the', 'votes', 'which', 'major', 'and']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IYwWhSBlRDqX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "54b5629e-e3ac-4bb6-ee62-0e6108e63534"
      },
      "cell_type": "code",
      "source": [
        "print(len(tess_tokens))\n",
        "print(len(ground_tokens))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "noBFlC6sjzSy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove\n",
        "tess_tokens = tess_tokens[:10000]\n",
        "ground_tokens = ground_tokens[:10000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PbgdRcXPRDqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = []\n",
        "for gt, tt in zip(ground_tokens, tess_tokens):\n",
        "        if gt == tt:\n",
        "            y.append(0)   # 0 indicates correct\n",
        "        else:\n",
        "            y.append(1)   # 1 indicates error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uIxhIzlfUw7r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(tess_tokens)   \n",
        "from sklearn.model_selection import train_test_split  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
        "ground_train = [ground_tokens[i] for i in X_train.index.tolist()]\n",
        "ground_test = [ground_tokens[i] for i in X_test.index.tolist()]\n",
        "X_train = X_train[0].tolist()\n",
        "X_test = X_test[0].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e6hxAoaAmSXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "48dd4a17-b340-469e-8e10-ce9cf71cdf60"
      },
      "cell_type": "code",
      "source": [
        "print(X_train[:30])\n",
        "print(ground_train[:30])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['percent', 'dlscusslon', '7', 'th-', 'to', '1n', 'soclety.\"', 'Cyanamld', 'our', 'Rage', 'and', 'sold', 'rlghts', 'unports', 'of', 'or', 'you', 'made', 'objectlons.', 'Many', 'CDALIT', 'operatlon.', 'one', 'Superfund', 'CAER.', 'prlmary', 'be', 'Board', 'Symuleskl', 'storles']\n",
            "['percent', 'discussion', '-', 'the', 'to', 'in', 'society.\"', 'Cyanamld', 'our', 'Rage', 'and', 'sold', 'rights', 'imports', 'of', 'or', 'you', 'made', 'objections.', 'Many', 'COALITION', 'operation,', 'one', 'excise', 'CAER.', 'primary', 'issued', 'Board', 'Symuleski', 'such']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "W4xL-rP0tgm_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define character type\n",
        "vowels = list('aeiou')\n",
        "consonants = list(\"bcdfghjklmnpqrstvwxyz\")\n",
        "digits = list('0123456789')\n",
        "\n",
        "def count_up(word):\n",
        "    u = [x for x in word if x.isupper()]\n",
        "    return len(u)\n",
        "\n",
        "def count_low(word):\n",
        "    l = [x for x in word if x.islower()]\n",
        "    return len(l)\n",
        "\n",
        "def count_cons_occur(word):  \n",
        "    max_count=0\n",
        "    count = 0\n",
        "    curr = ''\n",
        "    for c in word:\n",
        "        if curr == c:\n",
        "            count += 1\n",
        "        else:\n",
        "            max_count = max(max_count, count)\n",
        "            curr = c\n",
        "            count = 1\n",
        "    max_count = max(max_count, count)\n",
        "    return(max_count)\n",
        "\n",
        "def count_cons_occur_consonants(word):  \n",
        "    max_count=0\n",
        "    count = 0\n",
        "    curr = ''\n",
        "    for c in word:\n",
        "        if c in consonants:\n",
        "            if curr == c:\n",
        "                count += 1\n",
        "            else:\n",
        "                max_count = max(max_count, count)\n",
        "                curr = c\n",
        "                count = 1\n",
        "    max_count = max(max_count, count)\n",
        "    return(max_count)\n",
        "    \n",
        "def trim_word(word, from_start=1, from_end=1):\n",
        "    return word[from_start:len(word) - from_end]\n",
        "\n",
        "def unique(list1):  \n",
        "    # intilize a null list \n",
        "    unique_list = []      \n",
        "    # traverse for all elements \n",
        "    for x in list1: \n",
        "        # check if exists in unique_list or not \n",
        "        if x not in unique_list: \n",
        "            unique_list.append(x)\n",
        "            \n",
        "def safe_div(x,y):\n",
        "    if y == 0:\n",
        "        return 0\n",
        "    return x / y\n",
        "\n",
        "def get_bigram_freq(word, bi_dict):\n",
        "    word = word.lower()\n",
        "    bf = []\n",
        "    for i in range(len(word)-1):\n",
        "        key = word[i:i+2]\n",
        "        bf.append(bi_dict[key])\n",
        "    return(bf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zd39iCOdEfom",
        "scrolled": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### find LB in ground_truth, prepared for bigram features\n",
        "# ground_tokens of all words in lowercase\n",
        "lower_ground_tokens = []\n",
        "for tk in ground_tokens:\n",
        "    tkl_g = tk.lower()\n",
        "    lower_ground_tokens.append(tkl_g)\n",
        "\n",
        "# A dict of all bigram frequencies\n",
        "bigram_dict = collections.defaultdict(int)\n",
        "for tk_g in lower_ground_tokens:\n",
        "    for i in range(len(tk_g)-1):\n",
        "        key = tk_g[i:i+2]\n",
        "        bigram_dict[key] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8ld7b_1PMDPn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def feature_matrix(X):\n",
        "    ##### Feature extracting\n",
        "    # feature 1\n",
        "    length = []\n",
        "\n",
        "    # feature 2\n",
        "    vowels_count = []\n",
        "    consonants_count = []\n",
        "    quotients_v_l = []\n",
        "    quotients_c_l = []\n",
        "    quotients_v_c = []\n",
        "\n",
        "    # feature 3\n",
        "    nonalpha = []\n",
        "\n",
        "    # feature 4\n",
        "    digits = []\n",
        "    quotients_d_l = []\n",
        "\n",
        "    # feature 5\n",
        "    lowers = []\n",
        "    uppers = []\n",
        "    quotients_low_l = []\n",
        "    quotients_up_l = []\n",
        "\n",
        "    consecutive_occur = []\n",
        "    quotients_cons_l = []\n",
        "    quotients_la = [] # total number of vowels, consonants and digits\n",
        "    consonants_secut = []\n",
        "    fix_nonalpha = []\n",
        "\n",
        "    # feature bigram\n",
        "    bigr = []\n",
        "\n",
        "    most_freq = []\n",
        "    l2 = []\n",
        "    \n",
        "    for tk in X:\n",
        "        ### feature 1\n",
        "        l = len(tk)\n",
        "        length.append(l)\n",
        "\n",
        "        ### feature 2\n",
        "        tkl = tk.lower()\n",
        "        v_count = 0\n",
        "        for v in tkl:\n",
        "            if v in vowels:\n",
        "                v_count +=1\n",
        "        vowels_count.append(v_count)\n",
        "\n",
        "        c_count = 0\n",
        "        for c in tkl:\n",
        "            if c in consonants:\n",
        "                c_count +=1\n",
        "        consonants_count.append(c_count)\n",
        "\n",
        "        quotients_v_l.append(v_count/l)\n",
        "        quotients_c_l.append(c_count/l)\n",
        "        quot_v_c = safe_div(v_count,c_count)\n",
        "        quotients_v_c.append(quot_v_c)\n",
        "\n",
        "        ### feature 4\n",
        "        d_count = len([d for d in tk if d in digits])\n",
        "        digits.append(d_count)\n",
        "        quotients_d_l.append(d_count/l)\n",
        "\n",
        "        ### feature 3\n",
        "        s_count = len([s for s in tk if s not in vowels or consonants or digits])\n",
        "        nonalpha.append(s_count)\n",
        "\n",
        "        ### feature 5\n",
        "        low_count = count_low(tk)\n",
        "        lowers.append(low_count)\n",
        "        up_count = count_up(tk)\n",
        "        uppers.append(up_count)\n",
        "        quotients_low_l.append(low_count/l)\n",
        "        quotients_up_l.append(up_count/l)\n",
        "\n",
        "        ### feature 6\n",
        "        cons_occur_count = int(count_cons_occur(tk))\n",
        "        consecutive_occur.append(cons_occur_count)\n",
        "        if cons_occur_count >= 3:\n",
        "            quotients_cons_l.append(cons_occur_count/l)\n",
        "        else:\n",
        "            quotients_cons_l.append(0)\n",
        "\n",
        "        ### feature 7\n",
        "        la = v_count + c_count + d_count\n",
        "        if s_count > la:\n",
        "            quotients_la.append(1)\n",
        "        else:\n",
        "            quotients_la.append(0)\n",
        "\n",
        "        ### feature 8\n",
        "        consonance_cons_cccur_count = int(count_cons_occur_consonants(tk))\n",
        "        if consonance_cons_cccur_count >= 6:\n",
        "            consonants_secut.append(1)\n",
        "        else:\n",
        "            consonants_secut.append(0)\n",
        "\n",
        "        ### feature 9\n",
        "        tk_removed = trim_word(tk)\n",
        "        k_count = len([k for k in tk_removed if k not in vowels or consonants or digits])\n",
        "        if k_count >=3:\n",
        "            fix_nonalpha.append(1)\n",
        "        else:\n",
        "            fix_nonalpha.append(0)  \n",
        "\n",
        "        ### feature 10 bigram\n",
        "        bf = get_bigram_freq(tk, bigram_dict)\n",
        "\n",
        "        lower_tess_tokens = []\n",
        "        tkl_t = tk.lower()\n",
        "        lower_tess_tokens.append(tkl_t)\n",
        "\n",
        "        n = len(set(lower_tess_tokens))\n",
        "        big = (sum(bf)/10000)/n \n",
        "        bigr.append(big)\n",
        "\n",
        "        ### feature 11 most frequent symbol\n",
        "        i_count = Counter(tk).most_common(1)[0][1]\n",
        "        if i_count >=3:\n",
        "            most_freq.append(1)\n",
        "        else:\n",
        "            most_freq.append(0)\n",
        "\n",
        "        ### feature 12 Non-alphabetical symbols: nonalpha/total symbols\n",
        "        l1 = len([v for v in tk.lower() if v in vowels] + [c for c in tk.lower() if c in consonants])\n",
        "        l2_count = l - l1\n",
        "        quot_l2 = safe_div(l2_count,l1)\n",
        "        l2.append(quot_l2)  \n",
        "        \n",
        "    ##### construct a feature dataframe for SVM\n",
        "    df1 = pd.DataFrame({'length': length,\n",
        "                        'vowels': vowels_count,\n",
        "                        'consonants': consonants_count,\n",
        "                        'quot v/l': quotients_v_l,\n",
        "                        'quot c/l': quotients_c_l,\n",
        "                        'quot v/c': quotients_v_c,\n",
        "                        'nonalpha': nonalpha,\n",
        "                        'digits': digits,\n",
        "                        'quot d/l': quotients_d_l,\n",
        "                        'lowers': lowers,\n",
        "                        'uppers': uppers,\n",
        "                        'quot low/l': quotients_low_l,\n",
        "                        'quot up/l': quotients_up_l,\n",
        "                        'cons occur': consecutive_occur,\n",
        "                        'quot cons/l': quotients_cons_l,\n",
        "                        'quot la': quotients_la,\n",
        "                        'consonants_secut': consonants_secut,\n",
        "                        'fix_nonalpha': fix_nonalpha,\n",
        "                        'bigr': bigr,\n",
        "                        'most_freq' : most_freq,\n",
        "                        'l2': l2})\n",
        "    return df1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4qm7cN9RDqv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_feature_train = feature_matrix(X_train)\n",
        "X_feature_test = feature_matrix(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VD1UsNAORDq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "10e80420-0e61-4ab2-e803-5b79d9fb81d8"
      },
      "cell_type": "code",
      "source": [
        "svclassifier = SVC(kernel='rbf')  \n",
        "svclassifier.fit(X_feature_train, y_train)  "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
              "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "ZSwqJ770RDq4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#to predict\n",
        "y_pred = svclassifier.predict(X_feature_test)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XGh8N69KRDq8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1127
        },
        "outputId": "7226f691-c5a2-4de5-c051-8982172cf7bb"
      },
      "cell_type": "code",
      "source": [
        "df_output = pd.DataFrame({'tokens_tesseract':X_test,\n",
        "                          'Predict_by_SVM': y_pred})\n",
        "print(df_output[:100])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Predict_by_SVM tokens_tesseract\n",
            "0                0               or\n",
            "1                0               be\n",
            "2                0         proposed\n",
            "3                0             part\n",
            "4                0            year.\n",
            "5                1                7\n",
            "6                0         Natlonal\n",
            "7                0               of\n",
            "8                0               to\n",
            "9                0              the\n",
            "10               1                7\n",
            "11               0              all\n",
            "12               1        Vlrtually\n",
            "13               1        Indlcated\n",
            "14               1               15\n",
            "15               0              the\n",
            "16               1    Internatlonal\n",
            "17               0            piece\n",
            "18               0          created\n",
            "19               0          matters\n",
            "20               0               on\n",
            "21               0          program\n",
            "22               0              key\n",
            "23               0              and\n",
            "24               1               1n\n",
            "25               0              the\n",
            "26               1             a.m.\n",
            "27               0         Monopoly\n",
            "28               1        1nvolv1ng\n",
            "29               0              the\n",
            "..             ...              ...\n",
            "70               0           Annual\n",
            "71               1               -n\n",
            "72               0             Each\n",
            "73               0              for\n",
            "74               1         capaclty\n",
            "75               0           Energy\n",
            "76               1              011\n",
            "77               0               on\n",
            "78               0      appearances\n",
            "79               0          Perlod!\n",
            "80               1         Programs\n",
            "81               1            plan,\n",
            "82               1      settlements\n",
            "83               0          Norfolk\n",
            "84               0           tanker\n",
            "85               0              the\n",
            "86               1         preempt:\n",
            "87               0             chat\n",
            "88               0               by\n",
            "89               0               of\n",
            "90               0             open\n",
            "91               1        mendments\n",
            "92               0           actlon\n",
            "93               1          fundlng\n",
            "94               0              the\n",
            "95               0            offer\n",
            "96               0           export\n",
            "97               1     lssue/lssues\n",
            "98               0            year.\n",
            "99               0              for\n",
            "\n",
            "[100 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LgJ-6B2tRDrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "3d7fcf94-1bae-44f4-ddda-46725f2fe134"
      },
      "cell_type": "code",
      "source": [
        "##### evaluation\n",
        "#confustion Matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "print(confusion_matrix(y_test,y_pred))  \n",
        "print(classification_report(y_test,y_pred))  "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[998 129]\n",
            " [291 582]]\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.77      0.89      0.83      1127\n",
            "          1       0.82      0.67      0.73       873\n",
            "\n",
            "avg / total       0.79      0.79      0.79      2000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q7ezVSAnaity",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Error correction"
      ]
    },
    {
      "metadata": {
        "id": "2CktPwLtalAr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Given the detected word error, in order to find the best correction, we need to generating the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke some lexical-similarity measure between the misspelled string and the candidates or a probabilistic estimate of the likelihood of the correction to rank order the candidates. Here, we implement the positional binary digram method in the first reference paper. (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1672564}{positional binary digram)"
      ]
    },
    {
      "metadata": {
        "id": "-VEfK86-1Dd5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fn7sOOYDapFz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def keep_alphabet(tokens):\n",
        "    out = []\n",
        "    for l in tokens:\n",
        "        l = l.lower()\n",
        "        if l in set('abcdefghijklmnopqrstuvwxyz '):\n",
        "            out.append(l)\n",
        "    return ''.join(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UzE5s1dvnpAl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def char_to_index(x):\n",
        "    return ord(x)-ord('a')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBwd4B3oh6LL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def correct(word, digrams):\n",
        "  detect = 0\n",
        "  for each in digrams:\n",
        "    if matrix[char_to_index(word[each[0]])][char_to_index(word[each[1]])] == 0:\n",
        "      detect += 1\n",
        "      beta_each = set(each)\n",
        "      if detect == 1:\n",
        "        beta = beta_each\n",
        "      else:\n",
        "        beta = beta.intersection(beta_each)\n",
        "  if len(beta) == 1:\n",
        "    v_list = []\n",
        "    for i in beta:\n",
        "      position = i\n",
        "    for j in range(len(word)):\n",
        "      alpha_j = string.ascii_lowercase.index(word[j])\n",
        "      if j < position:\n",
        "        vector_j = digrams[(j, position)][alpha_j]\n",
        "        v_list.append(vector_j)\n",
        "      elif j > position:\n",
        "        vector_j = [item[alpha_j] for item in digrams[(position, j)]]\n",
        "        v_list.append(vector_j)\n",
        "    v = v_list[1]\n",
        "    for each in v_list:\n",
        "      v = v and each\n",
        "    if sum(v) == 1:\n",
        "      word[position] = string.ascii_lowercase[v.index(1)]\n",
        "  return word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vr1Bsyh1cPUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(ground_train)):\n",
        "  ground_train[i] = keep_alphabet(ground_train[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KztP5VHZcN2J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dictionary of digrams\n",
        "group_by_len = collections.defaultdict(list)\n",
        "digrams_by_len = collections.defaultdict(dict)\n",
        "for w in ground_train:\n",
        "  group_by_len[len(w)].append(w)\n",
        "for length in group_by_len:\n",
        "        for i, j in itertools.combinations(range(length), 2):\n",
        "                key = (i, j)\n",
        "                matrix = [[0] * 26 for _ in range(26)]\n",
        "                for words in group_by_len[length]:\n",
        "                    matrix[char_to_index(words[i])][char_to_index(words[j])] = 1\n",
        "                    digrams_by_len[length][key] = matrix  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7TSJth-EgTOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make correction\n",
        "corrected_test = X_test.copy()\n",
        "for i in range(len(corrected_test)):\n",
        "  corrected_test[i] = keep_alphabet(corrected_test[i])\n",
        "  \n",
        "for i in range(len(y_pred)):\n",
        "  if y_pred[i] == 1:\n",
        "    word_length = len(corrected_test[i])\n",
        "    if word_length > 1:\n",
        "      digrams_i = digrams_by_len[word_length]\n",
        "      corrected_test[i] = correct(corrected_test[i], digrams_i)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "towmW8Vkuj3-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_correct = []\n",
        "for gt, ct in zip(ground_test, corrected_test):\n",
        "        if gt == ct:\n",
        "            y_correct.append(0)   # 0 indicates correct\n",
        "        else:\n",
        "            y_correct.append(1)   # 1 indicates error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P2uE-TeO7bQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "6ce89dbb-2f39-4302-ddc6-3bedc1370f7f"
      },
      "cell_type": "code",
      "source": [
        "print(corrected_test[:30])\n",
        "print(ground_test[:30])\n",
        "print(X_test[:30])"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['or', 'be', 'proposed', 'part', 'year', '', 'natlonal', 'of', 'to', 'the', '', 'all', 'vlrtually', 'indlcated', '', 'the', 'internatlonal', 'piece', 'created', 'matters', 'on', 'program', 'key', 'and', 'n', 'the', 'am', 'monopoly', 'nvolvng', 'the']\n",
            "['or', 'be', 'proposed', 'part', 'year.', '-', 'National', 'of', 'to', 'the', '-', 'all', 'virtually', 'Indicated', 'is', 'the', 'International', 'piece', 'a', 'matters', 'on', 'program', 'key', 'and', 'in', 'the', 'a.m.', 'of', 'involving', 'the']\n",
            "['or', 'be', 'proposed', 'part', 'year.', '7', 'Natlonal', 'of', 'to', 'the', '7', 'all', 'Vlrtually', 'Indlcated', '15', 'the', 'Internatlonal', 'piece', 'created', 'matters', 'on', 'program', 'key', 'and', '1n', 'the', 'a.m.', 'Monopoly', '1nvolv1ng', 'the']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}