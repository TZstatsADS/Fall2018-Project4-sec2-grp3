{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project4_Detection_v4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "g1YWiD2WA1fN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Optical character recognition (OCR) \n",
        "Project 4 Group 3\n"
      ]
    },
    {
      "metadata": {
        "id": "rViGVJ0xRK5u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Error Detection"
      ]
    },
    {
      "metadata": {
        "id": "a0c7XybVBJrI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First of all, we need to detect errors, or incorrectly processed words. Here we extract features according to the paper and use SVM for garbage detection."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "yfF_19MM9yWY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3571
        },
        "outputId": "c7c5fc2c-6ff1-4bee-b4c4-0814bcd1a27c"
      },
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/ground_truth/\n",
            "  inflating: data/ground_truth/group1_00000005.txt  \n",
            "  inflating: data/ground_truth/group1_00000010.txt  \n",
            "  inflating: data/ground_truth/group1_00000013.txt  \n",
            "  inflating: data/ground_truth/group1_00000018.txt  \n",
            "  inflating: data/ground_truth/group1_00000021.txt  \n",
            "  inflating: data/ground_truth/group1_00000031.txt  \n",
            "  inflating: data/ground_truth/group1_00000035.txt  \n",
            "  inflating: data/ground_truth/group1_00000043.txt  \n",
            "  inflating: data/ground_truth/group1_00000049.txt  \n",
            "  inflating: data/ground_truth/group1_00000053.txt  \n",
            "  inflating: data/ground_truth/group2_00000004.txt  \n",
            "  inflating: data/ground_truth/group2_00000005.txt  \n",
            "  inflating: data/ground_truth/group2_00000015.txt  \n",
            "  inflating: data/ground_truth/group2_00000016.txt  \n",
            "  inflating: data/ground_truth/group2_00000017.txt  \n",
            "  inflating: data/ground_truth/group2_00000018.txt  \n",
            "  inflating: data/ground_truth/group2_00000034.txt  \n",
            "  inflating: data/ground_truth/group2_00000035.txt  \n",
            "  inflating: data/ground_truth/group2_00000036.txt  \n",
            "  inflating: data/ground_truth/group2_00000037.txt  \n",
            "  inflating: data/ground_truth/group2_00000042.txt  \n",
            "  inflating: data/ground_truth/group2_00000043.txt  \n",
            "  inflating: data/ground_truth/group2_00000047.txt  \n",
            "  inflating: data/ground_truth/group2_00000048.txt  \n",
            "  inflating: data/ground_truth/group2_00000050_1.txt  \n",
            "  inflating: data/ground_truth/group2_00000050_2.txt  \n",
            "  inflating: data/ground_truth/group2_00000055.txt  \n",
            "  inflating: data/ground_truth/group2_00000059.txt  \n",
            "  inflating: data/ground_truth/group2_00000060.txt  \n",
            "  inflating: data/ground_truth/group2_00000061.txt  \n",
            "  inflating: data/ground_truth/group2_00000062.txt  \n",
            "  inflating: data/ground_truth/group2_00000069.txt  \n",
            "  inflating: data/ground_truth/group2_00000070.txt  \n",
            "  inflating: data/ground_truth/group2_00000071.txt  \n",
            "  inflating: data/ground_truth/group2_00000080.txt  \n",
            "  inflating: data/ground_truth/group2_00000081.txt  \n",
            "  inflating: data/ground_truth/group2_00000096.txt  \n",
            "  inflating: data/ground_truth/group2_00000097.txt  \n",
            "  inflating: data/ground_truth/group3_00000043_1.txt  \n",
            "  inflating: data/ground_truth/group3_00000043_2.txt  \n",
            "  inflating: data/ground_truth/group3_00000043_3.txt  \n",
            "  inflating: data/ground_truth/group4_00000003_1.txt  \n",
            "  inflating: data/ground_truth/group4_00000003_2.txt  \n",
            "  inflating: data/ground_truth/group4_00000003_3.txt  \n",
            "  inflating: data/ground_truth/group4_00000003_4.txt  \n",
            "  inflating: data/ground_truth/group4_00000003_5.txt  \n",
            "  inflating: data/ground_truth/group4_00000003_6.txt  \n",
            "  inflating: data/ground_truth/group4_00000003_7.txt  \n",
            "  inflating: data/ground_truth/group4_00000006_1.txt  \n",
            "  inflating: data/ground_truth/group4_00000006_2.txt  \n",
            "  inflating: data/ground_truth/group4_00000006_3.txt  \n",
            "  inflating: data/ground_truth/group4_00000006_4.txt  \n",
            "  inflating: data/ground_truth/group4_00000006_5.txt  \n",
            "  inflating: data/ground_truth/group4_00000006_6.txt  \n",
            "  inflating: data/ground_truth/group4_00000006_7.txt  \n",
            "  inflating: data/ground_truth/group4_00000009_1.txt  \n",
            "  inflating: data/ground_truth/group4_00000009_2.txt  \n",
            "  inflating: data/ground_truth/group4_00000009_3.txt  \n",
            "  inflating: data/ground_truth/group4_00000009_4.txt  \n",
            "  inflating: data/ground_truth/group4_00000009_5.txt  \n",
            "  inflating: data/ground_truth/group4_00000009_6.txt  \n",
            "  inflating: data/ground_truth/group4_00000009_7.txt  \n",
            "  inflating: data/ground_truth/group4_00000009_8.txt  \n",
            "  inflating: data/ground_truth/group4_00000013_1.txt  \n",
            "  inflating: data/ground_truth/group4_00000013_2.txt  \n",
            "  inflating: data/ground_truth/group4_00000013_3.txt  \n",
            "  inflating: data/ground_truth/group4_00000013_4.txt  \n",
            "  inflating: data/ground_truth/group4_00000013_5.txt  \n",
            "  inflating: data/ground_truth/group5_00000003_1.txt  \n",
            "  inflating: data/ground_truth/group5_00000003_10.txt  \n",
            "  inflating: data/ground_truth/group5_00000003_2.txt  \n",
            "  inflating: data/ground_truth/group5_00000003_3.txt  \n",
            "  inflating: data/ground_truth/group5_00000003_4.txt  \n",
            "  inflating: data/ground_truth/group5_00000003_5.txt  \n",
            "  inflating: data/ground_truth/group5_00000003_6.txt  \n",
            "  inflating: data/ground_truth/group5_00000003_7.txt  \n",
            "  inflating: data/ground_truth/group5_00000003_8.txt  \n",
            "  inflating: data/ground_truth/group5_00000003_9.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_1.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_10.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_11.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_12.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_13.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_2.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_3.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_4.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_5.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_6.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_7.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_8.txt  \n",
            "  inflating: data/ground_truth/group5_00000009_9.txt  \n",
            "  inflating: data/ground_truth/group5_00000012_1.txt  \n",
            "  inflating: data/ground_truth/group5_00000012_2.txt  \n",
            "  inflating: data/ground_truth/group5_00000012_3.txt  \n",
            "  inflating: data/ground_truth/group5_00000012_4.txt  \n",
            "  inflating: data/ground_truth/group5_00000012_5.txt  \n",
            "  inflating: data/ground_truth/group5_00000012_6.txt  \n",
            "  inflating: data/ground_truth/group5_00000012_7.txt  \n",
            "  inflating: data/ground_truth/group5_00000012_8.txt  \n",
            "  inflating: data/ground_truth/group5_00000012_9.txt  \n",
            "  inflating: data/README.md          \n",
            "   creating: data/tesseract/\n",
            "  inflating: data/tesseract/group1_00000005.txt  \n",
            "  inflating: data/tesseract/group1_00000010.txt  \n",
            "  inflating: data/tesseract/group1_00000013.txt  \n",
            "  inflating: data/tesseract/group1_00000018.txt  \n",
            "  inflating: data/tesseract/group1_00000021.txt  \n",
            "  inflating: data/tesseract/group1_00000031.txt  \n",
            "  inflating: data/tesseract/group1_00000035.txt  \n",
            "  inflating: data/tesseract/group1_00000043.txt  \n",
            "  inflating: data/tesseract/group1_00000049.txt  \n",
            "  inflating: data/tesseract/group1_00000053.txt  \n",
            "  inflating: data/tesseract/group2_00000004.txt  \n",
            "  inflating: data/tesseract/group2_00000005.txt  \n",
            "  inflating: data/tesseract/group2_00000015.txt  \n",
            "  inflating: data/tesseract/group2_00000016.txt  \n",
            "  inflating: data/tesseract/group2_00000017.txt  \n",
            "  inflating: data/tesseract/group2_00000018.txt  \n",
            "  inflating: data/tesseract/group2_00000034.txt  \n",
            "  inflating: data/tesseract/group2_00000035.txt  \n",
            "  inflating: data/tesseract/group2_00000036.txt  \n",
            "  inflating: data/tesseract/group2_00000037.txt  \n",
            "  inflating: data/tesseract/group2_00000042.txt  \n",
            "  inflating: data/tesseract/group2_00000043.txt  \n",
            "  inflating: data/tesseract/group2_00000047.txt  \n",
            "  inflating: data/tesseract/group2_00000048.txt  \n",
            "  inflating: data/tesseract/group2_00000050_1.txt  \n",
            "  inflating: data/tesseract/group2_00000050_2.txt  \n",
            "  inflating: data/tesseract/group2_00000055.txt  \n",
            "  inflating: data/tesseract/group2_00000059.txt  \n",
            "  inflating: data/tesseract/group2_00000060.txt  \n",
            "  inflating: data/tesseract/group2_00000061.txt  \n",
            "  inflating: data/tesseract/group2_00000062.txt  \n",
            "  inflating: data/tesseract/group2_00000069.txt  \n",
            "  inflating: data/tesseract/group2_00000070.txt  \n",
            "  inflating: data/tesseract/group2_00000071.txt  \n",
            "  inflating: data/tesseract/group2_00000080.txt  \n",
            "  inflating: data/tesseract/group2_00000081.txt  \n",
            "  inflating: data/tesseract/group2_00000096.txt  \n",
            "  inflating: data/tesseract/group2_00000097.txt  \n",
            "  inflating: data/tesseract/group3_00000043_1.txt  \n",
            "  inflating: data/tesseract/group3_00000043_2.txt  \n",
            "  inflating: data/tesseract/group3_00000043_3.txt  \n",
            "  inflating: data/tesseract/group4_00000003_1.txt  \n",
            "  inflating: data/tesseract/group4_00000003_2.txt  \n",
            "  inflating: data/tesseract/group4_00000003_3.txt  \n",
            "  inflating: data/tesseract/group4_00000003_4.txt  \n",
            "  inflating: data/tesseract/group4_00000003_5.txt  \n",
            "  inflating: data/tesseract/group4_00000003_6.txt  \n",
            "  inflating: data/tesseract/group4_00000003_7.txt  \n",
            "  inflating: data/tesseract/group4_00000006_1.txt  \n",
            "  inflating: data/tesseract/group4_00000006_2.txt  \n",
            "  inflating: data/tesseract/group4_00000006_3.txt  \n",
            "  inflating: data/tesseract/group4_00000006_4.txt  \n",
            "  inflating: data/tesseract/group4_00000006_5.txt  \n",
            "  inflating: data/tesseract/group4_00000006_6.txt  \n",
            "  inflating: data/tesseract/group4_00000006_7.txt  \n",
            "  inflating: data/tesseract/group4_00000009_1.txt  \n",
            "  inflating: data/tesseract/group4_00000009_2.txt  \n",
            "  inflating: data/tesseract/group4_00000009_3.txt  \n",
            "  inflating: data/tesseract/group4_00000009_4.txt  \n",
            "  inflating: data/tesseract/group4_00000009_5.txt  \n",
            "  inflating: data/tesseract/group4_00000009_6.txt  \n",
            "  inflating: data/tesseract/group4_00000009_7.txt  \n",
            "  inflating: data/tesseract/group4_00000009_8.txt  \n",
            "  inflating: data/tesseract/group4_00000013_1.txt  \n",
            "  inflating: data/tesseract/group4_00000013_2.txt  \n",
            "  inflating: data/tesseract/group4_00000013_3.txt  \n",
            "  inflating: data/tesseract/group4_00000013_4.txt  \n",
            "  inflating: data/tesseract/group4_00000013_5.txt  \n",
            "  inflating: data/tesseract/group5_00000003_1.txt  \n",
            "  inflating: data/tesseract/group5_00000003_10.txt  \n",
            "  inflating: data/tesseract/group5_00000003_2.txt  \n",
            "  inflating: data/tesseract/group5_00000003_3.txt  \n",
            "  inflating: data/tesseract/group5_00000003_4.txt  \n",
            "  inflating: data/tesseract/group5_00000003_5.txt  \n",
            "  inflating: data/tesseract/group5_00000003_6.txt  \n",
            "  inflating: data/tesseract/group5_00000003_7.txt  \n",
            "  inflating: data/tesseract/group5_00000003_8.txt  \n",
            "  inflating: data/tesseract/group5_00000003_9.txt  \n",
            "  inflating: data/tesseract/group5_00000009_1.txt  \n",
            "  inflating: data/tesseract/group5_00000009_10.txt  \n",
            "  inflating: data/tesseract/group5_00000009_11.txt  \n",
            "  inflating: data/tesseract/group5_00000009_12.txt  \n",
            "  inflating: data/tesseract/group5_00000009_13.txt  \n",
            "  inflating: data/tesseract/group5_00000009_2.txt  \n",
            "  inflating: data/tesseract/group5_00000009_3.txt  \n",
            "  inflating: data/tesseract/group5_00000009_4.txt  \n",
            "  inflating: data/tesseract/group5_00000009_5.txt  \n",
            "  inflating: data/tesseract/group5_00000009_6.txt  \n",
            "  inflating: data/tesseract/group5_00000009_7.txt  \n",
            "  inflating: data/tesseract/group5_00000009_8.txt  \n",
            "  inflating: data/tesseract/group5_00000009_9.txt  \n",
            "  inflating: data/tesseract/group5_00000012_1.txt  \n",
            "  inflating: data/tesseract/group5_00000012_2.txt  \n",
            "  inflating: data/tesseract/group5_00000012_3.txt  \n",
            "  inflating: data/tesseract/group5_00000012_4.txt  \n",
            "  inflating: data/tesseract/group5_00000012_5.txt  \n",
            "  inflating: data/tesseract/group5_00000012_6.txt  \n",
            "  inflating: data/tesseract/group5_00000012_7.txt  \n",
            "  inflating: data/tesseract/group5_00000012_8.txt  \n",
            "  inflating: data/tesseract/group5_00000012_9.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iwpNpdFxUqll",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import collections\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "import itertools\n",
        "#import itertools\n",
        "#nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gw4gIzwQRDqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "3616cd5c-2bfd-40c3-f14d-181a99982a77"
      },
      "cell_type": "code",
      "source": [
        "##### read ground_truth\n",
        "ground_dir = glob.glob(os.path.join(os.getcwd(),'data/ground_truth','*.txt'))\n",
        "ground_tokens = []\n",
        "for gd in ground_dir:\n",
        "    with open(gd) as ground_file:\n",
        "        ground_raw = ground_file.read()\n",
        "        ground_t = ground_raw.split()   \n",
        "        ground_tokens += ground_t\n",
        "print(len(ground_tokens))\n",
        "print(ground_tokens[:20])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "294841\n",
            "['wa', 'reasonable,', 'although', 'it', 'substantially', 'increased', 'the', 'programs', 'and', 'authority', 'over', 'preset', 'law.', 'Following', 'is', 'a', 'brief', 'list', 'of', 'key']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zoq2wOeFRDqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tess_dir = glob.glob(os.path.join(os.getcwd(),'data/tesseract','*.txt'))\n",
        "#ground_dir = glob.glob(os.path.join(os.getcwd(),'data/ground_truth','*.txt'))\n",
        "file_name_gd = []\n",
        "file_name_td = []\n",
        "for gd, td in zip(ground_dir, tess_dir):\n",
        "        with open(gd, encoding=\"utf8\") as ground_file:    #, encoding=\"utf8\"\n",
        "            with open(td, encoding=\"utf8\") as tess_file:                \n",
        "                ground_r = list(ground_file.readlines()) \n",
        "                tess_r = list(tess_file.readlines())\n",
        "                if len(tess_r) == len(ground_r):\n",
        "                    file_name_td.append(td)\n",
        "                    file_name_gd.append(gd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rz5JErxMRDqU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make sure all the lines have the same size.\n",
        "ground_tokens=[]\n",
        "tess_tokens=[]\n",
        "for gd, td in zip(file_name_gd, file_name_td):\n",
        "        with open(gd) as file1:\n",
        "            with open(td) as file2:\n",
        "                for line1,line2 in zip(file1,file2):\n",
        "                    if len(line1)==len(line2):\n",
        "                        for word1,word2 in zip(line1.split(),line2.split()):\n",
        "                            ground_tokens.append(word1)\n",
        "                            tess_tokens.append(word2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xs2RfLUgXj6e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c4ace6b0-a38d-42a0-c5fe-0c412f0176eb"
      },
      "cell_type": "code",
      "source": [
        "print(tess_tokens[:30])\n",
        "print(ground_tokens[:30])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ha', 'reas-', 'able,', 'although', '1:', 'substantlally', '1ncreased', 'the', 'programs', 'and', 'authorlty', 'preset', '1-H.', 'Followl', '.', '15', 'a', 'brlef', '115:', 'of', 'key', 'lssues', 'that', 'were', 'consldered', 'and', 'the', 'whlch', 'm-Ior', 'and']\n",
            "['wa', 'reasonable,', 'although', 'it', 'substantially', 'increased', 'the', 'programs', 'and', 'authority', 'over', 'preset', 'law.', 'Following', 'is', 'a', 'brief', 'list', 'of', 'key', 'issues', 'that', 'were', 'considered', 'and', 'the', 'votes', 'which', 'major', 'and']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IYwWhSBlRDqX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "41f1bf1d-aad9-42e4-8b37-daaf9e123d8f"
      },
      "cell_type": "code",
      "source": [
        "print(len(tess_tokens))\n",
        "print(len(ground_tokens))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "152143\n",
            "152143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PbgdRcXPRDqc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = []\n",
        "for gt, tt in zip(ground_tokens, tess_tokens):\n",
        "        if gt == tt:\n",
        "            y.append(0)   # 0 indicates correct\n",
        "        else:\n",
        "            y.append(1)   # 1 indicates error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uIxhIzlfUw7r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = pd.DataFrame(tess_tokens)   \n",
        "from sklearn.model_selection import train_test_split  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
        "ground_train = [ground_tokens[i] for i in X_train.index.tolist()]\n",
        "ground_test = [ground_tokens[i] for i in X_test.index.tolist()]\n",
        "X_train = X_train[0].tolist()\n",
        "X_test = X_test[0].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e6hxAoaAmSXD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "5fa0fbc0-a836-4cea-e349-e9a31bd23eef"
      },
      "cell_type": "code",
      "source": [
        "print(X_train[:30])\n",
        "print(ground_train[:30])\n",
        "print(y_train[:30])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['type', '1n', '19557', 'Verltable', 'emlsslons', 'Hlth', 'Brussels', 'court,', 'are', 'organlzatlon.', 'for', 'truck', 'wlth', 'by', 'c', 'leglslatlon.', 'settlng', 'represent', 'on', 'and', 'program.', 'evaluatlng', 'at', 'necessary', 'on', 'on', 'the', '1nclude', 'Vice', 'be']\n",
            "['of', 'in', '1965)', 'veritable', 'emissions', 'with', 'Brussels', 'court,', 'are', 'organization.', 'for', 'truck', 'with', 'the', 'f', 'legislation.', 'setting', 'represent', 'on', 'and', 'program.', 'evaluating', 'at', 'necessary', 'on', 'On', 'the', 'include', 'Vice', 'be']\n",
            "[1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "W4xL-rP0tgm_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define character type\n",
        "vowels = list('aeiou')\n",
        "consonants = list(\"bcdfghjklmnpqrstvwxyz\")\n",
        "digits = list('0123456789')\n",
        "\n",
        "def count_up(word):\n",
        "    u = [x for x in word if x.isupper()]\n",
        "    return len(u)\n",
        "\n",
        "def count_low(word):\n",
        "    l = [x for x in word if x.islower()]\n",
        "    return len(l)\n",
        "\n",
        "def count_cons_occur(word):  \n",
        "    max_count=0\n",
        "    count = 0\n",
        "    curr = ''\n",
        "    for c in word:\n",
        "        if curr == c:\n",
        "            count += 1\n",
        "        else:\n",
        "            max_count = max(max_count, count)\n",
        "            curr = c\n",
        "            count = 1\n",
        "    max_count = max(max_count, count)\n",
        "    return(max_count)\n",
        "\n",
        "def count_cons_occur_consonants(word):  \n",
        "    max_count=0\n",
        "    count = 0\n",
        "    curr = ''\n",
        "    for c in word:\n",
        "        if c in consonants:\n",
        "            if curr == c:\n",
        "                count += 1\n",
        "            else:\n",
        "                max_count = max(max_count, count)\n",
        "                curr = c\n",
        "                count = 1\n",
        "    max_count = max(max_count, count)\n",
        "    return(max_count)\n",
        "    \n",
        "def trim_word(word, from_start=1, from_end=1):\n",
        "    return word[from_start:len(word) - from_end]\n",
        "\n",
        "def unique(list1):  \n",
        "    # intilize a null list \n",
        "    unique_list = []      \n",
        "    # traverse for all elements \n",
        "    for x in list1: \n",
        "        # check if exists in unique_list or not \n",
        "        if x not in unique_list: \n",
        "            unique_list.append(x)\n",
        "            \n",
        "def safe_div(x,y):\n",
        "    if y == 0:\n",
        "        return 0\n",
        "    return x / y\n",
        "\n",
        "def get_bigram_freq(word, bi_dict):\n",
        "    word = word.lower()\n",
        "    bf = []\n",
        "    for i in range(len(word)-1):\n",
        "        key = word[i:i+2]\n",
        "        bf.append(bi_dict[key])\n",
        "    return(bf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zd39iCOdEfom",
        "scrolled": true,
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### find LB in ground_truth, prepared for bigram features\n",
        "# ground_tokens of all words in lowercase\n",
        "lower_ground_tokens = []\n",
        "for tk in ground_tokens:\n",
        "    tkl_g = tk.lower()\n",
        "    lower_ground_tokens.append(tkl_g)\n",
        "\n",
        "# A dict of all bigram frequencies\n",
        "bigram_dict = collections.defaultdict(int)\n",
        "for tk_g in lower_ground_tokens:\n",
        "    for i in range(len(tk_g)-1):\n",
        "        key = tk_g[i:i+2]\n",
        "        bigram_dict[key] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8ld7b_1PMDPn",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def feature_matrix(X):\n",
        "    ##### Feature extracting\n",
        "    # feature 1\n",
        "    length = []\n",
        "\n",
        "    # feature 2\n",
        "    vowels_count = []\n",
        "    consonants_count = []\n",
        "    quotients_v_l = []\n",
        "    quotients_c_l = []\n",
        "    quotients_v_c = []\n",
        "\n",
        "    # feature 3\n",
        "    nonalpha = []\n",
        "\n",
        "    # feature 4\n",
        "    digits = []\n",
        "    quotients_d_l = []\n",
        "\n",
        "    # feature 5\n",
        "    lowers = []\n",
        "    uppers = []\n",
        "    quotients_low_l = []\n",
        "    quotients_up_l = []\n",
        "\n",
        "    consecutive_occur = []\n",
        "    quotients_cons_l = []\n",
        "    quotients_la = [] # total number of vowels, consonants and digits\n",
        "    consonants_secut = []\n",
        "    fix_nonalpha = []\n",
        "\n",
        "    # feature bigram\n",
        "    bigr = []\n",
        "\n",
        "    most_freq = []\n",
        "    l2 = []\n",
        "    \n",
        "    for tk in X:\n",
        "        ### feature 1\n",
        "        l = len(tk)\n",
        "        length.append(l)\n",
        "\n",
        "        ### feature 2\n",
        "        tkl = tk.lower()\n",
        "        v_count = 0\n",
        "        for v in tkl:\n",
        "            if v in vowels:\n",
        "                v_count +=1\n",
        "        vowels_count.append(v_count)\n",
        "\n",
        "        c_count = 0\n",
        "        for c in tkl:\n",
        "            if c in consonants:\n",
        "                c_count +=1\n",
        "        consonants_count.append(c_count)\n",
        "\n",
        "        quotients_v_l.append(v_count/l)\n",
        "        quotients_c_l.append(c_count/l)\n",
        "        quot_v_c = safe_div(v_count,c_count)\n",
        "        quotients_v_c.append(quot_v_c)\n",
        "\n",
        "        ### feature 4\n",
        "        d_count = len([d for d in tk if d in digits])\n",
        "        digits.append(d_count)\n",
        "        quotients_d_l.append(d_count/l)\n",
        "\n",
        "        ### feature 3\n",
        "        s_count = len([s for s in tk if s not in vowels or consonants or digits])\n",
        "        nonalpha.append(s_count)\n",
        "\n",
        "        ### feature 5\n",
        "        low_count = count_low(tk)\n",
        "        lowers.append(low_count)\n",
        "        up_count = count_up(tk)\n",
        "        uppers.append(up_count)\n",
        "        quotients_low_l.append(low_count/l)\n",
        "        quotients_up_l.append(up_count/l)\n",
        "\n",
        "        ### feature 6\n",
        "        cons_occur_count = int(count_cons_occur(tk))\n",
        "        consecutive_occur.append(cons_occur_count)\n",
        "        if cons_occur_count >= 3:\n",
        "            quotients_cons_l.append(cons_occur_count/l)\n",
        "        else:\n",
        "            quotients_cons_l.append(0)\n",
        "\n",
        "        ### feature 7\n",
        "        la = v_count + c_count + d_count\n",
        "        if s_count > la:\n",
        "            quotients_la.append(1)\n",
        "        else:\n",
        "            quotients_la.append(0)\n",
        "\n",
        "        ### feature 8\n",
        "        consonance_cons_cccur_count = int(count_cons_occur_consonants(tk))\n",
        "        if consonance_cons_cccur_count >= 6:\n",
        "            consonants_secut.append(1)\n",
        "        else:\n",
        "            consonants_secut.append(0)\n",
        "\n",
        "        ### feature 9\n",
        "        tk_removed = trim_word(tk)\n",
        "        k_count = len([k for k in tk_removed if k not in vowels or consonants or digits])\n",
        "        if k_count >=3:\n",
        "            fix_nonalpha.append(1)\n",
        "        else:\n",
        "            fix_nonalpha.append(0)  \n",
        "\n",
        "        ### feature 10 bigram\n",
        "        bf = get_bigram_freq(tk, bigram_dict)\n",
        "\n",
        "        lower_tess_tokens = []\n",
        "        tkl_t = tk.lower()\n",
        "        lower_tess_tokens.append(tkl_t)\n",
        "\n",
        "        n = len(set(lower_tess_tokens))\n",
        "        big = (sum(bf)/10000)/n \n",
        "        bigr.append(big)\n",
        "\n",
        "        ### feature 11 most frequent symbol\n",
        "        i_count = Counter(tk).most_common(1)[0][1]\n",
        "        if i_count >=3:\n",
        "            most_freq.append(1)\n",
        "        else:\n",
        "            most_freq.append(0)\n",
        "\n",
        "        ### feature 12 Non-alphabetical symbols: nonalpha/total symbols\n",
        "        l1 = len([v for v in tk.lower() if v in vowels] + [c for c in tk.lower() if c in consonants])\n",
        "        l2_count = l - l1\n",
        "        quot_l2 = safe_div(l2_count,l1)\n",
        "        l2.append(quot_l2)  \n",
        "        \n",
        "    ##### construct a feature dataframe for SVM\n",
        "    df1 = pd.DataFrame({'length': length,\n",
        "                        'vowels': vowels_count,\n",
        "                        'consonants': consonants_count,\n",
        "                        'quot v/l': quotients_v_l,\n",
        "                        'quot c/l': quotients_c_l,\n",
        "                        'quot v/c': quotients_v_c,\n",
        "                        'nonalpha': nonalpha,\n",
        "                        'digits': digits,\n",
        "                        'quot d/l': quotients_d_l,\n",
        "                        'lowers': lowers,\n",
        "                        'uppers': uppers,\n",
        "                        'quot low/l': quotients_low_l,\n",
        "                        'quot up/l': quotients_up_l,\n",
        "                        'cons occur': consecutive_occur,\n",
        "                        'quot cons/l': quotients_cons_l,\n",
        "                        'quot la': quotients_la,\n",
        "                        'consonants_secut': consonants_secut,\n",
        "                        'fix_nonalpha': fix_nonalpha,\n",
        "                        'bigr': bigr,\n",
        "                        'most_freq' : most_freq,\n",
        "                        'l2': l2})\n",
        "    return df1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4qm7cN9RDqv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_feature_train = feature_matrix(X_train)\n",
        "X_feature_test = feature_matrix(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VD1UsNAORDq0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "34b6487b-0fba-43ae-f87b-271c4e046e5e"
      },
      "cell_type": "code",
      "source": [
        "svclassifier = SVC(kernel='rbf')  \n",
        "svclassifier.fit(X_feature_train, y_train)  "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
              "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "  tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "7GuAblCa7eM0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "filename = 'svm_model.pkl'\n",
        "pickle.dump(svclassifier, open(filename, 'wb'))\n",
        "files.download('svm_model.pkl')\n",
        "# Can load the model from output or upload to colab\n",
        "# svm_model = pickle.load(open(filename, 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZSwqJ770RDq4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#to predict\n",
        "y_pred = svclassifier.predict(X_feature_test)  \n",
        "# y_pred = svm_model.predict(X_feature_test)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XGh8N69KRDq8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1127
        },
        "outputId": "f592fa21-538d-44c6-baaf-d065761ff038"
      },
      "cell_type": "code",
      "source": [
        "df_output = pd.DataFrame({'tokens_tesseract':X_test,\n",
        "                          'Predict_by_SVM': y_pred})\n",
        "print(df_output[:100])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Predict_by_SVM tokens_tesseract\n",
            "0                1               1n\n",
            "1                0               on\n",
            "2                0              and\n",
            "3                1      Washlngton,\n",
            "4                1                7\n",
            "5                1               1:\n",
            "6                0           Report\n",
            "7                1               40\n",
            "8                0          reached\n",
            "9                0            could\n",
            "10               1      Cozporatlon\n",
            "11               0              the\n",
            "12               0           recent\n",
            "13               0             Vent\n",
            "14               0          sesslon\n",
            "15               0              are\n",
            "16               0              and\n",
            "17               0             able\n",
            "18               0          between\n",
            "19               1        commodlty\n",
            "20               0              the\n",
            "21               0              Mr.\n",
            "22               1       Addltlonal\n",
            "23               0               to\n",
            "24               0         approach\n",
            "25               0                a\n",
            "26               1        Commlttee\n",
            "27               0             are:\n",
            "28               0            would\n",
            "29               0              but\n",
            "..             ...              ...\n",
            "70               1      wllllngness\n",
            "71               1               1n\n",
            "72               1               1n\n",
            "73               0          editing\n",
            "74               0            Clean\n",
            "75               0               to\n",
            "76               0            among\n",
            "77               0            \"Good\n",
            "78               1       condltlons\n",
            "79               0               on\n",
            "80               0           market\n",
            "81               1           Publlc\n",
            "82               1         alnlalz\"\n",
            "83               1          publlsh\n",
            "84               1        Kentucky,\n",
            "85               0              met\n",
            "86               0              the\n",
            "87               1             Th1:\n",
            "88               0              and\n",
            "89               1            steps\n",
            "90               1               15\n",
            "91               0              one\n",
            "92               0            money\n",
            "93               1       government\n",
            "94               0         proposed\n",
            "95               0          levels.\n",
            "96               0          release\n",
            "97               0              for\n",
            "98               0              the\n",
            "99               0              THE\n",
            "\n",
            "[100 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LgJ-6B2tRDrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "e4512c20-ce44-4c10-e203-e21bba73e5b5"
      },
      "cell_type": "code",
      "source": [
        "##### evaluation\n",
        "#confustion Matrix\n",
        "from sklearn.metrics import classification_report, confusion_matrix  \n",
        "print(confusion_matrix(y_test,y_pred))  \n",
        "print(classification_report(y_test,y_pred))  "
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[16297  2093]\n",
            " [ 3459  8580]]\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "          0       0.82      0.89      0.85     18390\n",
            "          1       0.80      0.71      0.76     12039\n",
            "\n",
            "avg / total       0.82      0.82      0.82     30429\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q7ezVSAnaity",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Error correction"
      ]
    },
    {
      "metadata": {
        "id": "2CktPwLtalAr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Given the detected word error, in order to find the best correction, we need to generating the candidate corrections: a dictionary or a database of legal n-grams to locate one or more potential correction terms. Then we need invoke some lexical-similarity measure between the misspelled string and the candidates or a probabilistic estimate of the likelihood of the correction to rank order the candidates. Here, we implement the positional binary digram method in the first reference paper. (https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1672564}{positional binary digram)"
      ]
    },
    {
      "metadata": {
        "id": "-VEfK86-1Dd5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fn7sOOYDapFz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def keep_alphabet(tokens):\n",
        "  # only retain alphabet\n",
        "  out = []\n",
        "  for l in tokens:\n",
        "      l = l.lower()\n",
        "      if l in set('abcdefghijklmnopqrstuvwxyz '):\n",
        "          out.append(l)\n",
        "  return ''.join(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zOWpBGeSeAzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Replace a postion of a string\n",
        "def replace_str_index(text,index=0,replacement=''):\n",
        "    return '%s%s%s'%(text[:index],replacement,text[index+1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBwd4B3oh6LL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the main correction function\n",
        "def correction(word, digrams):\n",
        "  detect = 0\n",
        "  beta = []\n",
        "  # Find the error positions\n",
        "  for each in digrams:\n",
        "    matrix = digrams[each]\n",
        "    if matrix[string.ascii_lowercase.index(word[each[0]])][string.ascii_lowercase.index(word[each[1]])] == 0:\n",
        "      detect += 1\n",
        "      beta_each = set(each)\n",
        "      if detect == 1:\n",
        "        beta = beta_each\n",
        "      else:\n",
        "        beta = beta.intersection(beta_each)\n",
        "  # print(beta)\n",
        "  # Consider when there is one or two elements in beta\n",
        "  if len(beta) in [1,2]:\n",
        "    choices = 0\n",
        "    v_dict = {}\n",
        "    for i in beta:\n",
        "      v_list = []\n",
        "      position = i\n",
        "      for j in range(len(word)):\n",
        "        alpha_j = string.ascii_lowercase.index(word[j])\n",
        "        if j < position:\n",
        "          vector_j = digrams[(j, position)][alpha_j]\n",
        "          v_list.append(vector_j)\n",
        "        elif j > position:\n",
        "          vector_j = [item[alpha_j] for item in digrams[(position, j)]]\n",
        "          v_list.append(vector_j)\n",
        "      v = v_list[0]\n",
        "      for each in v_list:\n",
        "        v = [a and b for a, b in zip(v, each)]\n",
        "      if sum(v) == 1:\n",
        "        choices += 1\n",
        "        v_dict[i] = v\n",
        "    if choices == 1:\n",
        "      for key in v_dict:\n",
        "        position_chosen = key\n",
        "      word = replace_str_index(word, position_chosen, string.ascii_lowercase[v_dict[position_chosen].index(1)])\n",
        "  return word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vr1Bsyh1cPUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(ground_train)):\n",
        "  ground_train[i] = keep_alphabet(ground_train[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KztP5VHZcN2J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create dictionary of digrams\n",
        "token_by_len = collections.defaultdict(list)\n",
        "digrams_by_len = collections.defaultdict(dict)\n",
        "for w in ground_train:\n",
        "  token_by_len[len(w)].append(w)\n",
        "  \n",
        "#print('Number of words of diffenrent length:')\n",
        "#for key, value in token_by_len.items() :\n",
        "#    print (key, len(value))\n",
        "\n",
        "for length in token_by_len:\n",
        "  for i, j in itertools.combinations(range(length), 2):\n",
        "    key = (i, j)\n",
        "    matrix = [[0] * 26 for _ in range(26)]\n",
        "    for words in token_by_len[length]:\n",
        "      matrix[string.ascii_lowercase.index(words[i])][string.ascii_lowercase.index(words[j])] = 1\n",
        "    digrams_by_len[length][key] = matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBWvfZZbVQCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "37ca3d07-b0b8-43e7-b024-43ed70a60e85"
      },
      "cell_type": "code",
      "source": [
        "# Number of 1 in digrams with given length\n",
        "ae = 0\n",
        "for each in digrams_by_len[10][(0,1)]:\n",
        "  ae += sum(each)\n",
        "print(ae)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sEmgfeS4KP5R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Clean the words\n",
        "corrected_test = X_test.copy()\n",
        "for i in range(len(corrected_test)):\n",
        "  corrected_test[i] = keep_alphabet(corrected_test[i])\n",
        "for i in range(len(ground_test)):\n",
        "  ground_test[i] = keep_alphabet(ground_test[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7TSJth-EgTOb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make correction\n",
        "for i in range(len(y_pred)):\n",
        "  if y_pred[i] == 1:\n",
        "    word_length = len(corrected_test[i])\n",
        "    if word_length > 1:\n",
        "      digrams_i = digrams_by_len[word_length]\n",
        "      corrected_test[i] = correction(corrected_test[i], digrams_i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "towmW8Vkuj3-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_corrected = []\n",
        "for gt, ct in zip(ground_test, corrected_test):\n",
        "        if gt == ct:\n",
        "            y_corrected.append(0)   # 0 indicates correct\n",
        "        else:\n",
        "            y_corrected.append(1)   # 1 indicates error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P2uE-TeO7bQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "57b2e4a2-e157-4aeb-e8ec-52df581fcb90"
      },
      "cell_type": "code",
      "source": [
        "# Compare the results\n",
        "print(corrected_test[:30])\n",
        "print(ground_test[:30])\n",
        "print(X_test[:30])"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['n', 'on', 'and', 'washlngton', '', '', 'report', '', 'reached', 'could', 'cozporatlon', 'the', 'recent', 'vent', 'sesslon', 'are', 'and', 'able', 'between', 'commodlty', 'the', 'mr', 'addltlonal', 'to', 'approach', 'a', 'commlttee', 'are', 'would', 'but']\n",
            "['in', 'on', 'and', 'washington', '', 'it', 'report', '', 'reached', 'could', 'corporation', 'the', 'recent', 'vent', 'session', 'are', 'and', 'able', 'governments', 'commodity', 'the', 'mr', 'additional', 'to', 'approach', 'a', 'committee', 'are', 'would', 'but']\n",
            "['1n', 'on', 'and', 'Washlngton,', '7', '1:', 'Report', '40', 'reached', 'could', 'Cozporatlon', 'the', 'recent', 'Vent', 'sesslon', 'are', 'and', 'able', 'between', 'commodlty', 'the', 'Mr.', 'Addltlonal', 'to', 'approach', 'a', 'Commlttee', 'are:', 'would', 'but']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RathHlusCCmv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0357315b-8faa-4676-abf4-1093d94898c4"
      },
      "cell_type": "code",
      "source": [
        "print(sum(y_test))\n",
        "print(sum(y_corrected))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12039\n",
            "10362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZVSjkKFiAwdQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Performance measure"
      ]
    },
    {
      "metadata": {
        "id": "o8QLnVn18-k7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create corresponding lists of characters\n",
        "corrected_char = []\n",
        "for each in corrected_test:\n",
        "  corrected_char += each\n",
        "corrected_char\n",
        "\n",
        "ground_char = []\n",
        "for each in ground_test:\n",
        "  ground_char += each\n",
        "\n",
        "tess_char = [] \n",
        "tess_test = X_test.copy()\n",
        "for i in range(len(tess_test)):\n",
        "  tess_test[i] = keep_alphabet(tess_test[i])\n",
        "for each in tess_test:\n",
        "  tess_char += each"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RN0kdSp_RoYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7swhDLPT-EZK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Count the number of characters\n",
        "ground_count = []\n",
        "corrected_count = []\n",
        "tess_count = []\n",
        "for each in string.ascii_lowercase:\n",
        "  ground_count.append(ground_char.count(each))\n",
        "  corrected_count.append(corrected_char.count(each))\n",
        "  tess_count.append(tess_char.count(each))\n",
        "ground_corrected_min = np.minimum(ground_count, corrected_count)\n",
        "ground_tess_min = np.minimum(ground_count, tess_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iH01NQqpAowo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "b2436256-1bf0-447e-b71d-28f14b99f950"
      },
      "cell_type": "code",
      "source": [
        "# Make the evaluation table\n",
        "word_recall_tess = 1 - sum(y_test)/len(ground_test)\n",
        "word_recall_corrected = 1 - sum(y_corrected)/len(ground_test)\n",
        "word_precision_tess = 1 - sum(y_test)/len(X_test)\n",
        "word_precision_corrected = 1 - sum(y_corrected)/len(X_test)\n",
        "char_recall_tess = sum(ground_tess_min)/sum(ground_count)\n",
        "char_precision_tess = sum(ground_tess_min)/sum(tess_count)\n",
        "char_recall_corrected = sum(ground_corrected_min)/sum(ground_count)\n",
        "char_precision_corrected = sum(ground_corrected_min)/sum(corrected_count)\n",
        "\n",
        "d = {'Tesseract': [word_recall_tess, word_precision_tess, char_recall_tess, char_precision_tess], \n",
        "     'Tesseract_with_postprocessing': [word_recall_corrected, word_precision_corrected \n",
        "                                       ,char_recall_corrected, char_precision_corrected]}\n",
        "OCR_performance_table = pd.DataFrame(data=d)\n",
        "OCR_performance_table.rename(index={0: 'word_wise_recall', 1:'word_wise_precision', \n",
        "                                    2:'character_wise_recall', 3:'character_wise_precision'}, inplace=True)\n",
        "print(OCR_performance_table)"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          Tesseract  Tesseract_with_postprocessing\n",
            "word_wise_recall           0.604358                       0.659470\n",
            "word_wise_precision        0.604358                       0.659470\n",
            "character_wise_recall      0.912326                       0.917584\n",
            "character_wise_precision   0.945590                       0.951040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kHvcr4LMA0VQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The word-wise recall after postprocessing is 0.6595, significantly better than original Tesseract text. Word-wise precision is the same as recall because of our preprocessing. The character-wise recall and precision are slightly improved to 0.9176 and 0.951."
      ]
    }
  ]
}